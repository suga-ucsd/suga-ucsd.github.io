<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>On the importance of Linux</title>
  <link rel="stylesheet" href="../index.css">
</head>
<body>

<header>
  <h1>2024-07-23 - Full artificial asistance - Devlog 0</h1>
  <p>2024-07-23</p>
  <a href="../blog.html">‚Üê Back to Blog</a>
</header>

<hr>

<article>
<div class="blog-content">
                            <img src="blog5.gif" class="blogpic">
                            <p>Now that state of the art model for LLM is open sourced with the emergence of Llama 3.1, I have finally started trusting LLMs. Zuck has won me over. Note how I did not use the word intelligence, because I whole heartedly believe, LLMs ARE NOT INTELLIGENCE (at least not yet). So them going rogue is an impossibility. Hence giving a *local* instance of an LLM is not an implausibilty for me. These turn of events today i.e. release of Llama 3.1 and making it open source. I am dreaming of a fully instant, elaboarate and extensive assistant. Humans prefer it when they work with someone else and discuss in real time with them. Now what if that discussion partner is an all knowing agent. This is what started the production of LLMs, but I feel the current LLM environment is still "unapprochable". I mean its still not what I imagine when I imagine an all-knowing partner would be. Firstly lets just make this clear, LLMs are not creative, they can only do what they can from the data they are trained on and result things that are in that data not outside. That is why they can never be intelligent and hence I dont mind giving it my data. Especially tsince it is a local instance and not connected to a companies server (Fine tuning is a different issue I will tackle later) </p>
                            <p>
                                My idea of an ideal assistant is that it knows my projects, my coding style, my paradigms, my knowledge, and has all the data on top of stuff it is already trained on. It should be able to assist me on the fly and learn my abilities continuously and suggest me stuff regarding my projects pretty accurately and the most important part being, it DOES NOT relay this information to the corresponding companies and now that it open source, I have this strong feeling of achieving this. I imagine myself writing code and my artificial assistant will instantly say my next steps or how I should approach. I should be able to ask it anything on the fly and it woud know the correct answer with respect my preference, but again a reminder it is all knowing so it will give the best advice. I imagine that I talk to the AA(artifical assistant) during my learning process or during my coding process, everything will happen isntantly. I imagine a future where everyt person has one fine tuned to their preferences instead of a very wide general model. I want to achieve this future now that SOTA is an open source model.
                            </p>
                            <p>
                                I expect this project to go on for years given I am poor and hence cant afford a GPU. But the fact that everything is pretty much planned would make the process much faster when there is an availibilty of resources. The current plan consists of several stages. First is using the Llama3 8b or Gemma 2 2b model as the starting of a fine tuning experiment. I need to first understand how the concept of fine tuning even works. Second stage is using this lightweight finetuned model and use it as copilot for code assitance. It will mostly give me suggestion sof the type of code I can write using a combination of natural language and code. Third stage will be the inclusion of images to this model. Inclusion does not mean that it will generate images (I think thats a different problem altogether, blog post soon ;P ), but rather analyse image given to it as an input. Fourth stage or stage 3b, mainly will allow the model in stage three to have access to my screen and thus a constant suggestions/assistance. I think this is the peak of the mountain right now as once I can overcome this stage, the next steps will be comparatively easier until a bigger mountain(voice suggestion??) comes. I do have an idea for linux as I can just give the outputs of the X display server, although not sure if that will be enough. But for windows it might be much heavy. Fifth stage is basically taking voice as input, its not as big of a problem as it might seem as, this research has been done for decades and models have made with pretty high accuracies to recognize each enunciations possible for each language (although I intend to stick with english for now). I dont wanna get ahead of myself just yet hence I would for now plan till fifth stage only.
                            </p>
                            <p>
                                Construction of a solid plan does not ensure adequeate implementation especially given the existence of ADHD in my system and constant warfare I am in with myself. But that does not demotivate me to stop making insane shit. My idoelogy is that it is 100% possible to fine tune an LLM model and make it your own. It would function as well with other people's input, it will be YOUR assistant. The speed of doing projects will just skyrocket. I am excited.
                            </p>
                        </div>
</article>

</body>
</html>


